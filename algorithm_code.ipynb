{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038409e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a0f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file_path: str) -> pd.DataFrame:\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91415b",
   "metadata": {},
   "source": [
    "***Prior Class*** ________________ **P(c) = Nc / N**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc09566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_count(dataset, unique_class, class_attribute):\n",
    "    \"\"\"\n",
    "        Returns number of occurrences of given class\n",
    "    \"\"\"\n",
    "    unique_class_column = dataset.loc[:,[class_attribute]]\n",
    "    class_count = 0\n",
    "    for _,row in unique_class_column.iterrows():\n",
    "        dataset_class = row[0]\n",
    "        if dataset_class == unique_class: \n",
    "            class_count += 1\n",
    "    return class_count\n",
    "\n",
    "def calculate_document_count(dataset):\n",
    "    document_count = len(dataset)\n",
    "    return document_count\n",
    "\n",
    "def calculate_class_probibility(unique_class_count, document_count): \n",
    "    \"\"\"\n",
    "        Returns probibility of given class\n",
    "        \n",
    "        Prior Class => P(c) = Nc / N\n",
    "    \"\"\"\n",
    "    class_probibility = unique_class_count / document_count\n",
    "    return class_probibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00944d73",
   "metadata": {},
   "source": [
    "***Liklihood***   ____________  **P(w |c) = count(w,c) + 1 / count(c) + |V|**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e730c59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_unique_tokens(testdata, tokens_attribute):\n",
    "    \"\"\"    \n",
    "        get all the words, make their set to get word's occuurrence once,\n",
    "        then returns all those in a list\n",
    "    \"\"\"\n",
    "    tokens_df = testdata.loc[:,[tokens_attribute]]\n",
    "    test_tokens_set = set()\n",
    "    for _,row in tokens_df.iterrows():\n",
    "        tokens = row[0].split(\" \")\n",
    "        for token in tokens:\n",
    "            test_tokens_set.add(token)\n",
    "    unique_test_tokens = list(test_tokens_set)\n",
    "    return (unique_test_tokens)\n",
    "\n",
    "\n",
    "def count_class_unique_token(dataset, unique_token, unique_class, unique_test_tokens, \\\n",
    "                                            tokens_attribute, class_attribute):\n",
    "    \"\"\"\n",
    "        Returns count of given token within class\n",
    "    \"\"\"\n",
    "    origional_data = dataset.loc[:,[tokens_attribute, class_attribute]]\n",
    "    token_count = 0\n",
    "    for _,row in origional_data.iterrows():\n",
    "        words_row = row[0].split(\" \")\n",
    "        df_class = row[1] \n",
    "        if df_class == unique_class:\n",
    "            \n",
    "            for token in words_row:\n",
    "                \n",
    "                if token in unique_test_tokens and token == unique_token:   \n",
    "                    token_count += 1\n",
    "    return token_count\n",
    "\n",
    "def count_total_class_tokens(dataset, unique_token, unique_class, tokens_attribute, class_attribute):\n",
    "    \"\"\"\n",
    "        Returns count of all the words in given class\n",
    "    \"\"\"\n",
    "    origional_data = dataset.loc[:,[tokens_attribute, class_attribute]]\n",
    "    tokens_count = 0\n",
    "#     filtered_data = origional_data[origional_data[\"Class\"] == unique_class]\n",
    "    for _,row in origional_data.iterrows():\n",
    "        words_row = row[0].split(\" \")\n",
    "        df_class = row[1] \n",
    "        if df_class == unique_class:\n",
    "            for token in words_row:  \n",
    "                tokens_count += 1\n",
    "    return tokens_count\n",
    "\n",
    "def count_vucabulary_tokens(dataset, tokens_attribute, class_attribute):\n",
    "    \"\"\"\n",
    "        get all words, make their set to get unique set, adds resultant set's occurrences and returns it\n",
    "    \"\"\"\n",
    "    origional_data = dataset.loc[:3,[tokens_attribute,class_attribute]]\n",
    "    tokens_set = set()\n",
    "    for _,row in origional_data.iterrows():\n",
    "        words_row = row[0].split(\" \")\n",
    "        for token in words_row: \n",
    "            tokens_set.add(token)\n",
    "    vucabulary_list = list(tokens_set)\n",
    "    vucabulary_count = 0\n",
    "    for i in vucabulary_list:\n",
    "        vucabulary_count += 1\n",
    "    \n",
    "    return vucabulary_count\n",
    "\n",
    "def calculate_hypothesis(class_unique_token_count, tokens_count, vucablary_count):\n",
    "    \"\"\"\n",
    "        Returns hypothesis of given class,\n",
    "        \n",
    "        Hypothesis Formula:\n",
    "        Liklihood => P(w |c) = count(w,c) + 1 / count(c) + |V|\n",
    "        \n",
    "        count(w,c): count the occurrencesof specific word in given class\n",
    "        count(c): count of all the words in given class\n",
    "        V: count of occurances of set of all words within dataset\n",
    "    \"\"\"\n",
    "    hypothesis = (class_unique_token_count + 1) / (total_class_tokens + vucablary_count)\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7e66a",
   "metadata": {},
   "source": [
    "***Maximum posterior***____ **P(c|d) = P(c|d) * P(c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a930a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_classes(dataset, class_attribute):\n",
    "    \"\"\"\n",
    "        Returns count of given word in test class\n",
    "    \"\"\"\n",
    "    sub_df = dataset.loc[:,[class_attribute]]\n",
    "    unique_classes = set()\n",
    "    for _,row in sub_df.iterrows():\n",
    "        distinct_class= row[0]\n",
    "        unique_classes.add(distinct_class)\n",
    "    unique_classes = list(unique_classes)\n",
    "    return unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcd789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_token_testdata(testdata, unique_token, tokens_attribute):\n",
    "    \"\"\"\n",
    "        Returns count of given word in testdata\n",
    "    \"\"\"\n",
    "    sub_testdata = testdata.loc[:,[tokens_attribute]]\n",
    "    unique_class_count = 0\n",
    "    for _,row in sub_testdata.iterrows():\n",
    "        test_tokens = row[0].split(\" \")\n",
    "        for token in test_tokens:\n",
    "            if token == unique_token:\n",
    "                unique_class_count += 1\n",
    "    return unique_class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba8c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_posterior(hypothesis, class_probibility, unique_test_tokens, test_class_unique_token_count):\n",
    "    \"\"\"\n",
    "        Returns posterior of a class \n",
    "        \n",
    "        Class Posterior => P(c|d) = P(w|c) * P(c)\n",
    "        \n",
    "        P(w|c): Product of all the hypothesis of words in given class\n",
    "        P(c): Probability of give class\n",
    "    \"\"\"\n",
    "    product_hypothesis_into_class_count = 1 \n",
    "    for word in unique_test_tokens:\n",
    "        hypothesis_into_class_count = hypothesis * test_class_unique_token_count\n",
    "        product_hypothesis_into_class_count *= hypothesis_into_class_count\n",
    "    class_posterior = class_probibility * product_hypothesis_into_class_count\n",
    "    return class_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb6416",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d1edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter class column name: Class\n",
      "Enter words column name: Words\n",
      "\n",
      "class_posterior of j:  0.0027434842249657062\n",
      "\n",
      "class_posterior of c:  0.0002733236151603498\n",
      "\n",
      "maximum_posterior:  j\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        This is the main entrypoint, orchestrating the Naive Bayes algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = read_dataset('dataset.csv')\n",
    "    testdata = read_dataset('testdata.csv')\n",
    "    class_attribute = input(\"Enter class column name: \")\n",
    "    tokens_attribute = input(\"Enter words column name: \")\n",
    "    \n",
    "    classes_posterior = {}\n",
    "    \n",
    "    unique_test_tokens = get_unique_tokens(testdata, tokens_attribute)\n",
    "    document_count = calculate_document_count(dataset) \n",
    "    unique_classes = get_unique_classes(dataset, class_attribute)\n",
    "    vucablary_count = count_vucabulary_tokens(dataset, tokens_attribute, class_attribute)\n",
    "    for unique_token in unique_test_tokens:\n",
    "        testdata_unique_token_count = count_unique_token_testdata(testdata, unique_token, tokens_attribute)\n",
    "    for unique_class in unique_classes:\n",
    "        unique_class_count = calculate_class_count(dataset, unique_class, class_attribute)\n",
    "        class_probibility = calculate_class_probibility(unique_class_count, document_count)\n",
    "        for unique_token in unique_test_tokens:\n",
    "            \n",
    "            class_unique_token_count = count_class_unique_token(dataset, unique_token, unique_class,\\\n",
    "                                                                         unique_test_tokens, tokens_attribute,\\\n",
    "                                                                         class_attribute)\n",
    "\n",
    "            total_class_tokens = count_total_class_tokens(dataset, unique_token, unique_class,\\\n",
    "                                                                tokens_attribute, class_attribute)  \n",
    "            hypothesis = calculate_hypothesis(class_unique_token_count,total_class_tokens,\\\n",
    "                                                                vucablary_count)\n",
    "\n",
    "        class_posterior = calculate_class_posterior(hypothesis, class_probibility, unique_test_tokens, testdata_unique_token_count)\n",
    "\n",
    "        print(f\"\\nclass_posterior of {unique_class}: \", class_posterior)\n",
    "        classes_posterior[unique_class] = class_posterior\n",
    "    maximum_posterior = max(classes_posterior)\n",
    "    print(\"\\nmaximum_posterior: \",maximum_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c3f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
